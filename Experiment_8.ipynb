{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYVOGwMjCHff",
        "outputId": "9d63bc69-a76f-49a5-c788-575c5920a7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import math\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Natural language processing is a field of artificial intelligence.\",\n",
        "    \"Bag of words is a simple text representation technique.\",\n",
        "    \"TF IDF gives importance to rare words in the document.\"\n",
        "]"
      ],
      "metadata": {
        "id": "4cxoTd9VCRN2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "print(\"Processed Documents:\")\n",
        "print(processed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xezlStbiCVX4",
        "outputId": "71c0de5f-8452-4db4-f28e-9595338a3a15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Documents:\n",
            "[['natural', 'language', 'processing', 'field', 'artificial', 'intelligence'], ['bag', 'words', 'simple', 'text', 'representation', 'technique'], ['tf', 'idf', 'gives', 'importance', 'rare', 'words', 'document']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "for doc in processed_docs:\n",
        "    vocab.update(doc)\n",
        "\n",
        "vocab = list(vocab)\n",
        "print(\"\\nVocabulary:\")\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjL-03T1CWsR",
        "outputId": "da7d5655-f2e3-4887-df0a-d1fce2072d04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary:\n",
            "['text', 'rare', 'processing', 'language', 'words', 'bag', 'document', 'intelligence', 'artificial', 'gives', 'simple', 'idf', 'technique', 'natural', 'importance', 'tf', 'field', 'representation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf = []\n",
        "\n",
        "for doc in processed_docs:\n",
        "    freq = FreqDist(doc)\n",
        "    doc_tf = []\n",
        "    for word in vocab:\n",
        "        doc_tf.append(freq[word] / len(doc))  # normalized TF\n",
        "    tf.append(doc_tf)\n",
        "\n",
        "print(\"\\nTF Matrix:\")\n",
        "print(tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ev1r92pCaYa",
        "outputId": "78520697-2ebb-4583-d007-fcad06d4676b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF Matrix:\n",
            "[[0.0, 0.0, 0.16666666666666666, 0.16666666666666666, 0.0, 0.0, 0.0, 0.16666666666666666, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.16666666666666666, 0.0], [0.16666666666666666, 0.0, 0.0, 0.0, 0.16666666666666666, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666], [0.0, 0.14285714285714285, 0.0, 0.0, 0.14285714285714285, 0.0, 0.14285714285714285, 0.0, 0.0, 0.14285714285714285, 0.0, 0.14285714285714285, 0.0, 0.0, 0.14285714285714285, 0.14285714285714285, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idf = []\n",
        "\n",
        "N = len(processed_docs)\n",
        "\n",
        "for word in vocab:\n",
        "    count = sum(1 for doc in processed_docs if word in doc)\n",
        "    idf.append(math.log(N / (count + 1)))  # smoothing\n",
        "\n",
        "print(\"\\nIDF Values:\")\n",
        "print(idf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_geipJfCctC",
        "outputId": "897d92db-158e-4a8f-f152-90c6c2663210"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IDF Values:\n",
            "[0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.0, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = []\n",
        "\n",
        "for doc_tf in tf:\n",
        "    doc_tfidf = []\n",
        "    for i in range(len(vocab)):\n",
        "        doc_tfidf.append(doc_tf[i] * idf[i])\n",
        "    tfidf.append(doc_tfidf)\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "for row in tfidf:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "RbTPLqFwCfXv",
        "outputId": "cd8d080d-80e7-4cfe-b47a-6be733560d75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            "[0.0, 0.0, 0.06757751801802739, 0.06757751801802739, 0.0, 0.0, 0.0, 0.06757751801802739, 0.06757751801802739, 0.0, 0.0, 0.0, 0.0, 0.06757751801802739, 0.0, 0.0, 0.06757751801802739, 0.0]\n",
            "[0.06757751801802739, 0.0, 0.0, 0.0, 0.0, 0.06757751801802739, 0.0, 0.0, 0.0, 0.0, 0.06757751801802739, 0.0, 0.06757751801802739, 0.0, 0.0, 0.0, 0.0, 0.06757751801802739]\n",
            "[0.0, 0.05792358687259491, 0.0, 0.0, 0.0, 0.0, 0.05792358687259491, 0.0, 0.0, 0.05792358687259491, 0.0, 0.05792358687259491, 0.0, 0.0, 0.05792358687259491, 0.05792358687259491, 0.0, 0.0]\n"
          ]
        }
      ]
    }
  ]
}